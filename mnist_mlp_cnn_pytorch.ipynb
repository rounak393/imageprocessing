{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPeayGeb0XCnkil+zUw5dtv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rounak393/imageprocessing/blob/main/mnist_mlp_cnn_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GZsJvZNeQTj",
        "outputId": "345db656-7828-4c26-c557-907b2027f703"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 38.1MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.19MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 10.8MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.35MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training MLP...\n",
            "Epoch 1/10 - Loss: 0.3866\n",
            "Epoch 2/10 - Loss: 0.1670\n",
            "Epoch 3/10 - Loss: 0.1248\n",
            "Epoch 4/10 - Loss: 0.1016\n",
            "Epoch 5/10 - Loss: 0.0873\n",
            "Epoch 6/10 - Loss: 0.0782\n",
            "Epoch 7/10 - Loss: 0.0711\n",
            "Epoch 8/10 - Loss: 0.0629\n",
            "Epoch 9/10 - Loss: 0.0591\n",
            "Epoch 10/10 - Loss: 0.0563\n",
            "Training finished.\n",
            "\n",
            "Classification Report for MLP:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9847    0.9867    0.9857       980\n",
            "           1     0.9861    0.9965    0.9912      1135\n",
            "           2     0.9749    0.9797    0.9773      1032\n",
            "           3     0.9649    0.9792    0.9720      1010\n",
            "           4     0.9641    0.9837    0.9738       982\n",
            "           5     0.9807    0.9686    0.9746       892\n",
            "           6     0.9874    0.9812    0.9843       958\n",
            "           7     0.9656    0.9815    0.9735      1028\n",
            "           8     0.9852    0.9600    0.9724       974\n",
            "           9     0.9837    0.9554    0.9693      1009\n",
            "\n",
            "    accuracy                         0.9776     10000\n",
            "   macro avg     0.9777    0.9772    0.9774     10000\n",
            "weighted avg     0.9777    0.9776    0.9776     10000\n",
            "\n",
            "\n",
            "Training CNN...\n",
            "Epoch 1/10 - Loss: 0.3161\n",
            "Epoch 2/10 - Loss: 0.1131\n",
            "Epoch 3/10 - Loss: 0.0882\n",
            "Epoch 4/10 - Loss: 0.0761\n",
            "Epoch 5/10 - Loss: 0.0706\n",
            "Epoch 6/10 - Loss: 0.0604\n",
            "Epoch 7/10 - Loss: 0.0567\n",
            "Epoch 8/10 - Loss: 0.0533\n",
            "Epoch 9/10 - Loss: 0.0455\n",
            "Epoch 10/10 - Loss: 0.0483\n",
            "Training finished.\n",
            "\n",
            "Classification Report for CNN:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9939    0.9949    0.9944       980\n",
            "           1     0.9965    0.9982    0.9974      1135\n",
            "           2     0.9904    0.9961    0.9932      1032\n",
            "           3     0.9970    0.9931    0.9950      1010\n",
            "           4     0.9949    0.9959    0.9954       982\n",
            "           5     0.9922    0.9933    0.9927       892\n",
            "           6     0.9937    0.9906    0.9922       958\n",
            "           7     0.9961    0.9864    0.9912      1028\n",
            "           8     0.9908    0.9959    0.9933       974\n",
            "           9     0.9931    0.9941    0.9936      1009\n",
            "\n",
            "    accuracy                         0.9939     10000\n",
            "   macro avg     0.9938    0.9938    0.9938     10000\n",
            "weighted avg     0.9939    0.9939    0.9939     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Install dependencies (if needed)\n",
        "!pip install torch torchvision scikit-learn --quiet\n",
        "\n",
        "# Step 2: Import required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Check GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Step 3: Define data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Converts to [0,1]\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize with mean/std of MNIST\n",
        "])\n",
        "\n",
        "# Step 4: Load dataset\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Split training into train and validation\n",
        "train_size = int(0.9 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_set, val_set = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=128, shuffle=True)\n",
        "val_loader   = DataLoader(val_set, batch_size=128)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=128)\n",
        "\n",
        "# Step 5A: Define MLP model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(28*28, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Step 6A: Train MLP\n",
        "def train_model(model, train_loader, val_loader, epochs=10):\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {running_loss/len(train_loader):.4f}\")\n",
        "    print(\"Training finished.\")\n",
        "\n",
        "def evaluate_model(model, data_loader):\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "            y_pred.extend(preds)\n",
        "            y_true.extend(labels.numpy())\n",
        "    return y_true, y_pred\n",
        "\n",
        "# Train and evaluate MLP\n",
        "print(\"\\nTraining MLP...\")\n",
        "mlp_model = MLP()\n",
        "train_model(mlp_model, train_loader, val_loader)\n",
        "y_true_mlp, y_pred_mlp = evaluate_model(mlp_model, test_loader)\n",
        "\n",
        "print(\"\\nClassification Report for MLP:\")\n",
        "print(classification_report(y_true_mlp, y_pred_mlp, digits=4))\n",
        "\n",
        "# Step 5B: Define CNN model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 7 * 7, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        return self.fc_layers(x)\n",
        "\n",
        "# Train and evaluate CNN\n",
        "print(\"\\nTraining CNN...\")\n",
        "cnn_model = CNN()\n",
        "train_model(cnn_model, train_loader, val_loader)\n",
        "y_true_cnn, y_pred_cnn = evaluate_model(cnn_model, test_loader)\n",
        "\n",
        "print(\"\\nClassification Report for CNN:\")\n",
        "print(classification_report(y_true_cnn, y_pred_cnn, digits=4))\n"
      ]
    }
  ]
}